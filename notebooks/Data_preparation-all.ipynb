{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples =  {'source_code': [],\n",
    "            'type': [],\n",
    "            'cbo': [],\n",
    "            'wmc': [],\n",
    "            'rfc': [],\n",
    "            'lcom': [],\n",
    "            'nom': [],\n",
    "            'nopm': [],\n",
    "            'nosm': [],\n",
    "            'nof': [],\n",
    "            'nopf': [],\n",
    "            'nosf': [],\n",
    "            'nosi': [],\n",
    "            'loc': [],\n",
    "            \"commits\": [],\n",
    "            \"linesAdded\": [],\n",
    "            \"linesDeleted\": [],\n",
    "            \"authors\": [],\n",
    "            \"minorAuthors\": [],\n",
    "            \"majorAuthors\": [],\n",
    "            \"authorOwnership\": [],\n",
    "            'target': []\n",
    "            }\n",
    "samples_ASTC =  {'source_code': [],\n",
    "            'type': [],\n",
    "            'cbo': [],\n",
    "            'wmc': [],\n",
    "            'rfc': [],\n",
    "            'lcom': [],\n",
    "            'nom': [],\n",
    "            'nopm': [],\n",
    "            'nosm': [],\n",
    "            'nof': [],\n",
    "            'nopf': [],\n",
    "            'nosf': [],\n",
    "            'nosi': [],\n",
    "            'loc': [],\n",
    "            \"commits\": [],\n",
    "            \"linesAdded\": [],\n",
    "            \"linesDeleted\": [],\n",
    "            \"authors\": [],\n",
    "            \"minorAuthors\": [],\n",
    "            \"majorAuthors\": [],\n",
    "            \"authorOwnership\": [],\n",
    "            'target': []\n",
    "            }\n",
    "\n",
    "metrics = ['cbo','wmc','rfc','lcom','nom','nopm','nosm','nof','nopf','nosf','nosi','loc']\n",
    "\n",
    "metrics2 = [\"commits\", \"linesAdded\", \"linesDeleted\", \"authors\", \"minorAuthors\", \"majorAuthors\", \"authorOwnership\"]\n",
    "metrics2_fake = {k:None for k in metrics2}\n",
    "\n",
    "def get_values(row):\n",
    "    metrics_values = {}\n",
    "    for m in metrics2:\n",
    "        metrics_values[m] = row[m]\n",
    "    return metrics_values\n",
    "\n",
    "\n",
    "def tokenize_code(code):\n",
    "    return ' '.join(nltk.tokenize.wordpunct_tokenize(code))\n",
    "\n",
    "def process_content(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        source_code = []\n",
    "        for line in lines:\n",
    "            # filter comments\n",
    "            if not re.match(\"\\s*\\/\\/\\s*isComment\", line):\n",
    "                source_code.append(tokenize_code(line))\n",
    "    text = ' '.join(source_code)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_apache = os.listdir(\"../data/zips/apache\")\n",
    "dirs_apache = [f for f in dirs_apache if f != '.DS_Store']\n",
    "dirs_fdroid = os.listdir(\"../data/zips/fdroid\")\n",
    "dirs_fdroid = [f for f in dirs_fdroid if f != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldirs_apache = []\n",
    "for d in dirs_apache:\n",
    "    if '.zip' not in d:\n",
    "        dirs = os.listdir(\"../data/zips/apache/{}/output\".format(d))\n",
    "        dirs = [di for di in dirs if di != '.DS_Store']\n",
    "        finaldirs_apache.append(\"../data/zips/apache/{}/output/{}\".format(d, dirs[0]))\n",
    "print(len(finaldirs_apache))\n",
    "\n",
    "finaldirs_fdroid = []\n",
    "for d in dirs_fdroid:\n",
    "    if '.zip' not in d:\n",
    "        dirs = os.listdir(\"../data/zips/fdroid/{}/output\".format(d))\n",
    "        dirs = [di for di in dirs if di != '.DS_Store']\n",
    "        finaldirs_fdroid.append(\"../data/zips/fdroid/{}/output/{}\".format(d, dirs[0]))\n",
    "print(len(finaldirs_fdroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repo_groups = [(finaldirs_apache, 'apache'), (finaldirs_fdroid, 'fdroid')]\n",
    "for group_repo in repo_groups:\n",
    "    for i, d in enumerate(group_repo[0]):\n",
    "        print(\"{} from {}\".format(i, len(group_repo[0])))\n",
    "        look_up = {}\n",
    "        with open('{}/process.csv'.format(d), 'r') as pro_file:\n",
    "            pro_csv = csv.DictReader(pro_file, delimiter=';')\n",
    "            for row in pro_csv:\n",
    "                look_up[\"{}_{}\".format(row[\"commit\"], row[\"file\"])] = get_values(row)\n",
    "\n",
    "        with open('{}/yes-refactoring.csv'.format(d), 'r') as pos_file:\n",
    "            pos_csv = csv.DictReader(pos_file, delimiter=';')\n",
    "            for row in pos_csv:\n",
    "                try:\n",
    "                    samples['source_code'].append(process_content(\"{}/storage/{}/before/{}\".format(d,row['before'],row['path'])))\n",
    "                except UnicodeDecodeError:\n",
    "                    samples['source_code'].append(None)\n",
    "                samples['type'].append(row['type'])\n",
    "                samples['target'].append(row['refactoring'])\n",
    "                for m in metrics:\n",
    "                    samples[m].append(row[m])\n",
    "                try:\n",
    "                    metrics2 = look_up[\"{}_{}\".format(row[\"before\"], row[\"path\"])]\n",
    "                    for k, v in metrics2.items():\n",
    "                        samples[k].append(v)\n",
    "                except KeyError:\n",
    "                    for k, v in metrics2_fake.items():\n",
    "                        samples[k].append(v)\n",
    "                    \n",
    "\n",
    "        with open('{}/no-refactoring.csv'.format(d), 'r') as neg_file:\n",
    "            neg_csv = csv.DictReader(neg_file, delimiter=';')\n",
    "            for row in neg_csv:\n",
    "                try:\n",
    "                    samples['source_code'].append(process_content(\"{}/storage/{}/not-refactored/{}\".format(d,row['commit'],row['path'])))\n",
    "                except UnicodeDecodeError:\n",
    "                    samples['source_code'].append(None)\n",
    "                samples['type'].append(row['type'])\n",
    "                samples['target'].append(\"NO_REFACTORING\")\n",
    "                for m in metrics:\n",
    "                    samples[m].append(row[m])\n",
    "\n",
    "                try:\n",
    "                    metrics2 = look_up[\"{}_{}\".format(row[\"before\"], row[\"path\"])]\n",
    "                    for k, v in metrics2.items():\n",
    "                        samples[k].append(v)\n",
    "                except KeyError:\n",
    "                    for k, v in metrics2_fake.items():\n",
    "                        samples[k].append(v)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(samples)\n",
    "    df.head()\n",
    "\n",
    "    df.to_pickle('../data/instances_{}.pkl'.format(group_repo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_groups = [(finaldirs_apache, 'apache'), (finaldirs_fdroid, 'fdroid')]\n",
    "for group_repo in repo_groups:\n",
    "    for i, d in enumerate(group_repo[0]):\n",
    "        print(\"{} from {}\".format(i, len(group_repo[0])))\n",
    "        look_up = {}\n",
    "        with open('{}/process.csv'.format(d), 'r') as pro_file:\n",
    "            pro_csv = csv.DictReader(pro_file, delimiter=';')\n",
    "            for row in pro_csv:\n",
    "                look_up[\"{}_{}\".format(row[\"commit\"], row[\"file\"])] = get_values(row)\n",
    "\n",
    "        with open('{}/yes-refactoring.csv'.format(d), 'r') as pos_file:\n",
    "            pos_csv = csv.DictReader(pos_file, delimiter=';')\n",
    "            for row in pos_csv:\n",
    "\n",
    "                samples['source_code'].append(process_content(\"{}/astconverter/storage/{}/before/{}\".format(d,row['before'],row['path'])))\n",
    "                samples['type'].append(row['type'])\n",
    "                samples['target'].append(row['refactoring'])\n",
    "                for m in metrics:\n",
    "                    samples[m].append(row[m])\n",
    "                try:\n",
    "                    metrics2 = look_up[\"{}_{}\".format(row[\"before\"], row[\"path\"])]\n",
    "                    for k, v in metrics2.items():\n",
    "                        samples[k].append(v)\n",
    "                except KeyError:\n",
    "                    for k, v in metrics2_fake.items():\n",
    "                        samples[k].append(v)\n",
    "                    \n",
    "\n",
    "        with open('{}/no-refactoring.csv'.format(d), 'r') as neg_file:\n",
    "            neg_csv = csv.DictReader(neg_file, delimiter=';')\n",
    "            for row in neg_csv:\n",
    "                samples['source_code'].append(process_content(\"{}/astconverter/storage/{}/not-refactored/{}\".format(d,row['commit'],row['path'])))\n",
    "                samples['type'].append(row['type'])\n",
    "                samples['target'].append(\"NO_REFACTORING\")\n",
    "                for m in metrics:\n",
    "                    samples[m].append(row[m])\n",
    "\n",
    "                try:\n",
    "                    metrics2 = look_up[\"{}_{}\".format(row[\"before\"], row[\"path\"])]\n",
    "                    for k, v in metrics2.items():\n",
    "                        samples[k].append(v)\n",
    "                except KeyError:\n",
    "                    for k, v in metrics2_fake.items():\n",
    "                        samples[k].append(v)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(samples)\n",
    "    df.head()\n",
    "\n",
    "    df.to_pickle('../data/instances_astconverter_{}.pkl'.format(group_repo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
