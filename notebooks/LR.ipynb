{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from time import time\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content_list):\n",
    "    source_code = []\n",
    "    content_list = eval(content_list)\n",
    "    for line in content_list:\n",
    "        # filter comments\n",
    "        if not re.match(\"\\s*\\/\\/\\s*isComment\", line):\n",
    "            source_code.append(line.replace(\"\\n\", \" newLine \"))\n",
    "    return ' '.join(source_code)\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "param_dist = {'C': scipy.stats.expon(scale=100)}\n",
    "\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(lr_classifier,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== tmm ==============\n",
      "Reading data...\n",
      "Preparing lists...\n",
      "Extracting features...\n",
      "Hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 151.05 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.632 (std: 0.025)\n",
      "Parameters: {'C': 226.21584447040712}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.632 (std: 0.025)\n",
      "Parameters: {'C': 279.83659756747403}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.632 (std: 0.025)\n",
      "Parameters: {'C': 302.523804331414}\n",
      "\n",
      "============ EVALUATION on test set:\n",
      "0.6135338345864662\n",
      "===== lc ==============\n",
      "Reading data...\n",
      "Preparing lists...\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 1.95 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 28.108601949536823}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 20.586448400482407}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 40.63622226326734}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 18.449166701479623}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 49.67448386029197}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 6.488245122086524}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 4.768536532889151}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 52.2620638897252}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 20.78853711584202}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.800 (std: 0.150)\n",
      "Parameters: {'C': 32.26968855656235}\n",
      "\n",
      "============ EVALUATION on test set:\n",
      "0.46402877697841727\n",
      "===== dc ==============\n",
      "Reading data...\n",
      "Preparing lists...\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 7.00 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.889 (std: 0.028)\n",
      "Parameters: {'C': 6.711407183845483}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.889 (std: 0.029)\n",
      "Parameters: {'C': 11.513372919994314}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.888 (std: 0.027)\n",
      "Parameters: {'C': 5.391865337022511}\n",
      "\n",
      "============ EVALUATION on test set:\n",
      "0.8405063291139241\n",
      "===== lpl ==============\n",
      "Reading data...\n",
      "Preparing lists...\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 0.48 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 21.316923787052225}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 4.983156233561143}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 42.900458853222126}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 28.328174907451185}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 32.214205486055704}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.944 (std: 0.051)\n",
      "Parameters: {'C': 3.671103087822675}\n",
      "\n",
      "============ EVALUATION on test set:\n",
      "0.680327868852459\n",
      "===== lm ==============\n",
      "Reading data...\n",
      "Preparing lists...\n",
      "Extracting features...\n",
      "Hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/Users/erickmaziero/virtualenvs/smells-dataset_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 57.40 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.650 (std: 0.055)\n",
      "Parameters: {'C': 8.427064659721047}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.647 (std: 0.057)\n",
      "Parameters: {'C': 16.80893997157763}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.647 (std: 0.057)\n",
      "Parameters: {'C': 17.63232694380791}\n",
      "\n",
      "============ EVALUATION on test set:\n",
      "0.6809895833333334\n"
     ]
    }
   ],
   "source": [
    "labels = [\"tmm\", \"lc\", \"dc\", \"lpl\", \"lm\"]\n",
    "for label in labels:\n",
    "    print(\"===== {} ==============\".format(label))\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv('data/df/train_{}.csv'.format(label), engine=\"python\")\n",
    "    df_test = pd.read_csv('data/df/test_{}.csv'.format(label), engine=\"python\")\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    print(\"Preparing lists...\")\n",
    "    for index, row in df.iterrows():\n",
    "        X_train.append(process_content(row[\"content\"]))\n",
    "        Y_train.append(row[\"smells\"])\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        X_test.append(process_content(row[\"content\"]))\n",
    "        Y_test.append(row[\"smells\"])\n",
    "\n",
    "    print(\"Extracting features...\")\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    cv.fit(X_train)\n",
    "    train_instances = cv.transform(X_train)\n",
    "    test_instances = cv.transform(X_test)\n",
    "    \n",
    "    X_t, X_v, y_t, y_v = train_test_split(train_instances, Y_train, train_size = 0.75)\n",
    "\n",
    "    start = time()\n",
    "    print(\"Hyperparameter tuning...\")\n",
    "    random_search.fit(train_instances, Y_train)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "    report(random_search.cv_results_)\n",
    "    print(\"============ EVALUATION on test set:\")\n",
    "    print(accuracy_score(Y_test, random_search.best_estimator_.predict(test_instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
